from flask import Flask, request, jsonify
import asyncio
from crawl4ai import AsyncWebCrawler
from tenacity import retry, stop_after_attempt, wait_fixed
import re
import traceback
import threading

app = Flask(__name__)
crawl_lock = threading.Lock()  # ğŸ”’ å…¨åŸŸé–

# ğŸ§¹ æ¸…æ´— markdown
def clean_markdown(raw_text: str) -> str:
    raw_text = re.sub(r"\[!\[.*?\]\(.*?\)\]\(javascript:.*?\)", "", raw_text)
    raw_text = re.sub(r"!\[\]\(.*?\)", "", raw_text)
    raw_text = re.split(r"##\s*é€£çµ¡è©¢å•", raw_text)[0]
    raw_text = re.sub(r"\[([^\]]*?)\]\((?:https?|javascript).*?\)", r"\1", raw_text)
    raw_text = re.sub(r"(https?|ftp):\/\/[^\s\)\]\}]+", "", raw_text)
    raw_text = re.sub(r"\.concat\([^\)]*\)", "", raw_text)
    raw_text = re.sub(r"encodeURIComponent\([^\)]*\)", "", raw_text)
    raw_text = re.sub(r"document\.title", "", raw_text)
    raw_text = re.sub(r"javascript:[^ \n\)]*", "", raw_text)
    raw_text = re.sub(r"[\\]?\)+[;)]*", "", raw_text)
    raw_text = re.sub(r"^\s*\*\s*$", "", raw_text, flags=re.MULTILINE)
    raw_text = re.sub(r"\n{2,}", "\n\n", raw_text)
    raw_text = re.sub(r" {2,}", " ", raw_text)
    h2_start = raw_text.find("# ")
    if h2_start != -1:
        raw_text = raw_text[h2_start:]
    return raw_text.strip()

# ğŸ” éåŒæ­¥çˆ¬èŸ²
@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
async def crawl4ai_with_retry(url: str) -> str:
    try:
        print(f"[DEBUG] é–‹å§‹ AsyncWebCrawler æŠ“å–ï¼š{url}")
        async with AsyncWebCrawler(strategy="httpx", verbose=True) as crawler:
            result = await crawler.arun(url=url)
            print(f"[DEBUG] æŠ“å–æˆåŠŸï¼Œé–‹å§‹æ¸…æ´—")
            return clean_markdown(result.markdown)
    except Exception as e:
        print(f"[ERROR] çˆ¬èŸ²å…§éƒ¨ä¾‹å¤–ï¼š{type(e).__name__} - {e}")
        raise

# ğŸ“¬ API ç«¯é»ï¼ˆåŠ å…¥é–æ§ï¼‰
@app.route('/crawl4ai_once', methods=['POST'])
def crawl4ai_once():
    data = request.get_json(force=True)
    url = data.get("url")

    if not url:
        return jsonify({"error": "Missing 'url'"}), 400

    with crawl_lock:  # ğŸ”’ ç¢ºä¿åŒæ™‚é–“åªè·‘ä¸€å€‹ request
        try:
            print(f"[DEBUG] åŸ·è¡Œ asyncio.run çˆ¬èŸ²ï¼š{url}")
            cleaned = asyncio.run(crawl4ai_with_retry(url))
            return jsonify({"markdown": cleaned})
        except Exception as e:
            print(f"[ERROR] ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            traceback.print_exc()
            return jsonify({"error": f"Crawl failed: {type(e).__name__} - {e}"}), 500

# ğŸš€ æœ¬åœ°ç«¯å•Ÿå‹•ç”¨
if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8000)